---
title: "Housing3_Week8_TatreauGillian"
author: "Gillian Tatreau"
date: "2022-10-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Code, echo=TRUE}
# import packages
library(readxl)
library(ggplot2)
library(plyr)
library(pastecs)
library(ggplot2)
library(dplyr)
library(QuantPsyc)
library(car)

# file source, read file
file_source <- "/Users/gillian/Documents/Bellevue Grad Program/Fall 2022/DSC520/DSC520 Repo/week-6-housing.xlsx"
housing <- read_excel(file_source)

# remove spaces in column names
names(housing) <- gsub(" ", "_", names(housing))
colnames(housing)

# Bathrooms
housing$bathrooms <- with(housing, bath_full_count + (bath_half_count * 0.5) + (bath_3qtr_count * 0.75))

# Sale Year
housing$Sale_Year <- format(housing$Sale_Date, format="%Y")

# Outside space
housing$sq_ft_outside <- with(housing, sq_ft_lot - square_feet_total_living)

# log transform of sale price
housing$log_price <- log(housing$Sale_Price)
stat.desc(housing$log_price, options(digits = 2))
ggplot(data = housing) + geom_histogram(aes(x = log_price), bins = 15) + xlab("Sale Price w/log transform") + ylab("Frequency")
ggplot(data = housing) + geom_histogram(aes(x = Sale_Price), bins = 15) + xlab("Sale Price w/o log transform") + ylab("Frequency")

# log transform of square feet
housing$log_sqft <- log(housing$square_feet_total_living)
stat.desc(housing$log_sqft, options(digits = 2))
ggplot(data = housing) + geom_histogram(aes(x = log_sqft), bins = 15) + xlab("Square Feet w/log transform") + ylab("Frequency")
ggplot(data = housing) + geom_histogram(aes(x = square_feet_total_living), bins = 15) + xlab("Square Feet w/o log transform") + ylab("Frequency")

# variables
price <- housing$log_price
year1 <- housing$Sale_Year
sqft <- housing$log_sqft
bed <- housing$bedrooms
bath <- housing$bathrooms
year2 <- housing$year_built

# simple regression
simple_lm <- lm(Sale_Price ~ square_feet_total_living, data = housing)
summary(simple_lm)
price_predict_df <- data.frame(sale_price = predict(simple_lm), square_feet=housing$square_feet_total_living)
ggplot(data = housing, aes(y = Sale_Price, x = square_feet_total_living)) +
  geom_point(color='blue') +
  geom_line(color='red',data = price_predict_df, aes(y=sale_price, x=square_feet))

# with log
simplelog_lm <- lm(price ~ sqft, data = housing)
summary(simplelog_lm)
pricelog_predict_df <- data.frame(logsale_price = predict(simplelog_lm), logsquare_feet=sqft)
ggplot(data = housing, aes(y = price, x = sqft)) +
  geom_point(color='blue') +
  geom_line(color='red',data = pricelog_predict_df, aes(y=logsale_price, x=logsquare_feet))

# correlation to see which variables to use as predictors
cor(price, sqft)
cor(price, bed)
cor(price, bath)
cor(price, as.numeric(year1))
cor(price, year2)
cor(price, as.numeric(housing$Sale_Date))
cor(price, housing$sq_ft_outside)


# multiple regression
multiple_lm <- lm(price ~ sqft + bed + bath + year2 + as.numeric(year1), data = housing)
summary(multiple_lm)
mult_price_predict <- data.frame(
  sale_price = predict(multiple_lm),
  square_feet=sqft, bed=bed, bath=bath,
  year2=year2, year1 = as.numeric(year1))


# standardized betas
lm.beta(multiple_lm)

# confidence intervals
confint(multiple_lm)

# analysis of variance
anova(simplelog_lm, multiple_lm)

# case-wise diagnostics for outliers/influential cases
housing$residuals <- resid(multiple_lm)
housing$stan_resid <- rstandard(multiple_lm)
housing$cooks <- cooks.distance(multiple_lm)
housing$leverage <- hatvalues(multiple_lm)
housing$covariance <- covratio(multiple_lm)
head(housing[c("residuals", "stan_resid", "cooks", "leverage", "covariance")], n=10)

# large residuals
housing$large_resid <- housing$stan_resid > 2 | housing$stan_resid < -2
head(housing$large_resid, n = 30)

# sum of large residuals
sum(housing$large_resid)

# which variables have large residuals
housing[housing$large_resid, c("log_price", "log_sqft", "Sale_Year", "bedrooms", "bathrooms", "year_built", "stan_resid")]

# leverage, Cook's distance, covariance ratios, k = 5
housing[housing$large_resid, c("leverage", "cooks", "covariance")]
housing$cooksprob <- housing$cooks > 1
avelev <- 6 / 12865
housing$badlev <- housing$leverage > (3 * avelev)
highcov <- 1 + ((3 * 6) / 12865)
lowcov <- 1 - ((3 * 6) / 12865)
housing$covissue <- housing$covariance > highcov | housing$covariance < lowcov
sum(housing$covissue)
sum(housing$badlev)
sum(housing$cooksprob)

housing[housing$large_resid, c("badlev", "cooksprob", "covissue")]

# assumption of independence
durbinWatsonTest(multiple_lm)

# assumption of no multicollinearity
vif(multiple_lm)
mean(vif(multiple_lm))
1/vif(multiple_lm)

# assumptions related to residuals
housing$fitted <- multiple_lm$fitted.values

plot(multiple_lm)
hist(housing$stan_resid)

```

## i. I used a log transform on both the Sale_Price and square_feet_total_living columns in order to make the distributions of that data more normal as well as more linear. I also added the variables bathrooms (the total number of bathrooms including half and three quarter bathrooms), Sale_Year (the year the house was sold), and sq_ft_outside which was meant to calculate the square footage of any outside space of the property. 
```{r Log transforms of price and sq_ft (histograms), echo=FALSE}
# log transform of sale price
housing$log_price <- log(housing$Sale_Price)
stat.desc(housing$log_price, options(digits = 2))
ggplot(data = housing) + geom_histogram(aes(x = log_price), bins = 15) + xlab("Sale Price w/log transform") + ylab("Frequency")
ggplot(data = housing) + geom_histogram(aes(x = Sale_Price), bins = 15) + xlab("Sale Price w/o log transform") + ylab("Frequency")

# log transform of square feet
housing$log_sqft <- log(housing$square_feet_total_living)
stat.desc(housing$log_sqft, options(digits = 2))
ggplot(data = housing) + geom_histogram(aes(x = log_sqft), bins = 15) + xlab("Square Feet w/log transform") + ylab("Frequency")
ggplot(data = housing) + geom_histogram(aes(x = square_feet_total_living), bins = 15) + xlab("Square Feet w/o log transform") + ylab("Frequency")
```

```{r comparison of simple LM with and without log transform, echo=FALSE}
# simple regression
simple_lm <- lm(Sale_Price ~ square_feet_total_living, data = housing)
summary(simple_lm)
price_predict_df <- data.frame(sale_price = predict(simple_lm), square_feet=housing$square_feet_total_living)
ggplot(data = housing, aes(y = Sale_Price, x = square_feet_total_living)) +
  geom_point(color='blue') +
  geom_line(color='red',data = price_predict_df, aes(y=sale_price, x=square_feet))

# with log
simplelog_lm <- lm(price ~ sqft, data = housing)
summary(simplelog_lm)
pricelog_predict_df <- data.frame(logsale_price = predict(simplelog_lm), logsquare_feet=sqft)
ggplot(data = housing, aes(y = price, x = sqft)) +
  geom_point(color='blue') +
  geom_line(color='red',data = pricelog_predict_df, aes(y=logsale_price, x=logsquare_feet))

```


## ii. For the predictors I chose, I started with what I thought would make sense intuitively and then checked the correlations for the variables that made the most sense. 

```{r simplelog_lm, echo=FALSE}
simplelog_lm <- lm(price ~ sqft, data = housing)
summary(simplelog_lm)
pricelog_predict_df <- data.frame(logsale_price = predict(simplelog_lm), logsquare_feet=sqft)
ggplot(data = housing, aes(y = price, x = sqft)) +
  geom_point(color='blue') +
  geom_line(color='red',data = pricelog_predict_df, aes(y=logsale_price, x=logsquare_feet))
```
```{r Correlations, echo=TRUE}
# correlation to see which variables to use as predictors
cor(price, sqft)
cor(price, bed)
cor(price, bath)
cor(price, as.numeric(year1))
cor(price, year2)
cor(price, as.numeric(housing$Sale_Date))
cor(price, housing$sq_ft_outside)
```

```{r multiple_lm, echo=FALSE}
# multiple regression
multiple_lm <- lm(price ~ sqft + bed + bath + year2 + as.numeric(year1), data = housing)
summary(multiple_lm)
mult_price_predict <- data.frame(
  sale_price = predict(multiple_lm),
  square_feet=sqft, bed=bed, bath=bath,
  year2=year2, year1 = as.numeric(year1))
```

## iii. The r^2 for the simplelog_lm is 0.249. The r^2 for the multiple_lm is 0.269. The adjusted r^2 is also 0.269, which means the cross-validity of this model is good. The model with more variables is slightly better at explaining the variations in the data.  
```{r summary of simplelog_lm, echo=FALSE}
summary(simplelog_lm)
```
```{r summary of multiple_lm, echo=FALSE}
summary(multiple_lm)
```

## iv. In this model, the most important predicter is the square_feet_total_living, followed by the year the house was built, then year the house was sold. The number of bathrooms and number of bedrooms have a similar degree of importance below the rest of the others. 
```{r standardized betas, echo=FALSE}
# standardized betas
lm.beta(multiple_lm)

```

## v. The confidence intervals for square feet, year built, and year sold are all very tight, indicating that these parameters are both representative of the true population and also significant. The confidence interval for bedrooms is more braod but is still significant, while less representative of the population. The confidence interval for bathrooms switches signs, which means that it is very unrepresentative of the population as well as not being a prticularly good predictor of the sale price. 
```{r confidence intervals, echo=FALSE}
# confidence intervals
confint(multiple_lm)
```

## vi. We can say that the multiple_lm model significantly improved the fit of the model as compared to the simplelog_lm, F(4, 12859) = 89.3, p < 0.001. 
```{r anova, echo=FALSE}
# analysis of variance
anova(simplelog_lm, multiple_lm)

```

## vii. 
```{r casewise diagnostics, echo=TRUE}
# case-wise diagnostics for outliers/influential cases
housing$residuals <- resid(multiple_lm)
housing$stan_resid <- rstandard(multiple_lm)
housing$cooks <- cooks.distance(multiple_lm)
housing$leverage <- hatvalues(multiple_lm)
housing$covariance <- covratio(multiple_lm)
head(housing[c("residuals", "stan_resid", "cooks", "leverage", "covariance")], n=10)
```

## viii. 
```{r large residuals, echo=TRUE}
# large residuals
housing$large_resid <- housing$stan_resid > 2 | housing$stan_resid < -2
head(housing$large_resid, n = 30)
```

## ix. 
```{r sum of large residuals, echo=TRUE}
# sum of large residuals
sum(housing$large_resid)
```

## x. 
```{r variables with large residuals, echo=TRUE}
# which variables have large residuals
housing[housing$large_resid, c("log_price", "log_sqft", "Sale_Year", "bedrooms", "bathrooms", "year_built", "stan_resid")]
```

## xi. None of the cases with large standard residuals have a Cook's distance greater than 1, so none of the observations have an undue influence on the model. Some of the observations have large leverage values, which means that these observations have a large influence on the outcome variables. There are also a great many observations with large covariance ratios. However, since none of these observations have a Cook's distance greater than 1, there is no need to delete them or exclude them from the model, but they can be studied in order to try to understand why they did not fit the model. 
```{r leverage, cooks distance, covariance ratios, echo=TRUE}
# leverage, Cook's distance, covariance ratios, k = 5
housing[housing$large_resid, c("leverage", "cooks", "covariance")]
housing$cooksprob <- housing$cooks > 1
avelev <- 6 / 12865
housing$badlev <- housing$leverage > (3 * avelev)
highcov <- 1 + ((3 * 6) / 12865)
lowcov <- 1 - ((3 * 6) / 12865)
housing$covissue <- housing$covariance > highcov | housing$covariance < lowcov
sum(housing$covissue)
sum(housing$badlev)
sum(housing$cooksprob)
housing[housing$large_resid, c("badlev", "cooksprob", "covissue")]
```

## xii. The model does not meet the assumption of independence. 
```{r assumption of independence, echo=FALSE}
# assumption of independence
durbinWatsonTest(multiple_lm)

```

## xiii. The VIF, then the mean of the VIF, then the tolerance is listed below. The average VIF is larger than 1 (at 1.8) which could suggest some bias in the model but since none of the VIFs are larger than 10 and the tolerances are all greater than 0.2, there is likely not too much collinearity within the data. 
```{r no multicollinearity, echo=FALSE}
# assumption of no multicollinearity
vif(multiple_lm)
mean(vif(multiple_lm))
1/vif(multiple_lm)
```
 ## xiv. There is some evidence of heteroscedasticity in the data, and the residuals are not normally distributed, with an over-dispersed distribution. The histogram proves there is an obvious left-skew of the standardized residuals. 
```{r assumptions relating to residuals, echo=FALSE}
plot(multiple_lm)
hist(housing$stan_resid)
```

## xv. The regression model is not entirely unbiased- there is some evidence of that the residuals are not random, homoscedasticity is not met. The model also does not meet the assumption of independence. Therefore, we are unable to make accurate assumptions or predictions about the population based off the model. Our predictions would have a propensity of being erroneous or innacurate when applied to the overall population. 
